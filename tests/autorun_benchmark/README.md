# Automated Benchmark Testing Process

## Setup
**Note:** when building all code, we recommend release versions with optimise for speed (-O3).

1. Install dependent Python packages (optionally creating a pyenv environment) by installing packages in _requirements.txt_.
2. Generate CHERI versions of all WASM and Native files for both hybrid-cap and pure-cap by runing _cheri_build.sh_ in each of the folders in _../tests/benchmarks_.  Make this available on the machine running the automated benchmark test script.
3. Build WAMR for both hybrid-cap and pure-cap, and make these two versions available on the Morello machine.
4. Build wamrc and make this available either on the localhost or a remote machine to which you have SSH access
5. Using the template _example_wamr_cheri_benchmark_settings.json_ as a guide, produce a JSON file to be used for the automated benchmark run.  This provides login information and folders to access your Morello computer and optionally another Linux macine to run wamrc:
- Requires SSH keypair to access remote machines
- _wamrc_host_ key entry is only needed if a remote machine must be used to build AOT with wamrc
- Modify fields "hostname", "user", "key", "dest" to fit your setup
- "wamr_purecap", "wamr_hybrid", "wamrc" point to iwasms and wamrc on the relevant host, modify as necessary

## Run
Run:

```
autorun_benchmark.py [-h] -f FOLDER -o OUTPUT_CSV [--c CONFIG] [-aot] [-native] [-r REP_COUNT] [-v] [-l LOGFILE]
```
Arguments:  
  -h, --help            show help message and exit  
  -f FOLDER, --folder FOLDER Folder containing benchmark WASMs (required)  
  -o OUTPUT_CSV, --output OUTPUT_CSV CSV File to save results to (required)  
  --c CONFIG, --config CONFIG JSON Config filename, default loads from OS environment setting "WAMR_CHERI_BENCHMARK_JSON"  
  -aot                  Run AOT tests (default false)  
  -native               Run Native tests (default false)  

Note that:
1. Folder containing WASMs is that which was prepared in setup
2. JSON Config filename was prepared as described in setup; this can be specified by setting the environment variable _WAMR_CHERI_BENCHMARK_JSON_ or passed on the command line
3. Repeat count will repeat each test the given number of times, and average results for each run.
4. To run WASM only, omit "-aot" and "-native" arguments

The full run will take many hours so disable any power saving which may place your computer into a hibernation mode.
On completion of the run, a CSV file is created which will - for each test - list the time taken to execute the test for:
- WASM (Hybrid)
- WASM (Purecap)
- AOT (Hybrid - if AOT run requested)
- AOT (Purecap - if AOT run requested)
- Native (Hybrid - if Native run requested)
- Native (Purecap - if Native run requested)

**Note**: A time of zero indicates the run failed (on purecap native this can occur because the native code is not ported for purecap and so generates a capability fault)

## Post-processing
The raw results CSV generated by _autorun_benchmark.py_ can be processed in order to generate the difference between hybrid vs purecap execution time, as a percentage of the hybrid execution time (+ve = hybrid faster, -ve = purecap faster).
These are generated for each category, namely:
- WASM hybrid vs purecap  
- AOT hybrid vs purecap  
- Native hybrid vs purecap  

A CSV can be generated showing the above for each test case individually, or summarised for each category (e.g all polybench tests) by averaging the individual results.

To generate, run:
```
csv_processor.py [-h] -i INPUT -o OUTPUT [--s] [-v]
```

Arguments:  
  -h, --help            show help message and exit  
  -i INPUT, --input INPUT Input raw CSV file (required)  
  -o OUTPUT, --output OUTPUT Summary CSV File to save output to (required)  
  --s, --summary        Generate Summary Only  
  -v                    Verbose logging (default minimal logging)  

Note that:
1. Input CSV is the one generated by _autorun_benchmark.py_
2. To generate percent speedups for each individual tets case, omit the _summary_ flag.  To average results on a per-category basis, include this flag.
